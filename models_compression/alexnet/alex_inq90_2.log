nohup: ignoring input
I1223 13:21:37.519207 27366 caffe.cpp:218] Using GPUs 0, 1, 2, 3
I1223 13:21:37.520038 27366 caffe.cpp:223] GPU 0: Tesla P40
I1223 13:21:37.520467 27366 caffe.cpp:223] GPU 1: Tesla P40
I1223 13:21:37.520869 27366 caffe.cpp:223] GPU 2: Tesla P40
I1223 13:21:37.521256 27366 caffe.cpp:223] GPU 3: Tesla P40
I1223 13:21:38.176002 27366 solver.cpp:44] Initializing solver from parameters: 
test_iter: 250
test_interval: 500
base_lr: 0.005
display: 25
max_iter: 100000
lr_policy: "exp"
gamma: 0.99994481
momentum: 0.9
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "models_compression/alexnet/inq90_2"
solver_mode: GPU
device_id: 0
net: "models_compression/alexnet/train_val_inq90_2.prototxt"
train_state {
  level: 0
  stage: ""
}
rampup_interval: 1000
rampup_lr: 5e-06
I1223 13:21:38.176371 27366 solver.cpp:87] Creating training net from net file: models_compression/alexnet/train_val_inq90_2.prototxt
I1223 13:21:38.177398 27366 net.cpp:301] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1223 13:21:38.177436 27366 net.cpp:301] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top1
I1223 13:21:38.177443 27366 net.cpp:301] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy_top5
I1223 13:21:38.177786 27366 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/data/caffe-imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/data/caffe-imagenet/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "INQConvolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "INQConvolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "INQConvolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "INQConvolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "INQConvolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "INQInnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_inner_product_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "INQInnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_inner_product_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "INQInnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  inq_inner_product_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1223 13:21:38.177966 27366 layer_factory.hpp:77] Creating layer data
I1223 13:21:38.178117 27366 db_lmdb.cpp:35] Opened lmdb /home/data/caffe-imagenet/ilsvrc12_train_lmdb
I1223 13:21:38.178174 27366 net.cpp:84] Creating Layer data
I1223 13:21:38.178184 27366 net.cpp:387] data -> data
I1223 13:21:38.178215 27366 net.cpp:387] data -> label
I1223 13:21:38.178234 27366 data_transformer.cpp:25] Loading mean file from: /home/data/caffe-imagenet/imagenet_mean.binaryproto
I1223 13:21:38.183853 27366 data_layer.cpp:45] output data size: 256,3,227,227
I1223 13:21:38.610461 27366 net.cpp:127] Setting up data
I1223 13:21:38.610505 27366 net.cpp:136] Top shape: 256 3 227 227 (39574272)
I1223 13:21:38.610514 27366 net.cpp:136] Top shape: 256 (256)
I1223 13:21:38.610519 27366 net.cpp:144] Memory required for data: 158298112
I1223 13:21:38.610533 27366 layer_factory.hpp:77] Creating layer conv1
I1223 13:21:38.610561 27366 net.cpp:84] Creating Layer conv1
I1223 13:21:38.610569 27366 net.cpp:413] conv1 <- data
I1223 13:21:38.610589 27366 net.cpp:387] conv1 -> conv1
I1223 13:21:38.614287 27366 net.cpp:127] Setting up conv1
I1223 13:21:38.614306 27366 net.cpp:136] Top shape: 256 96 55 55 (74342400)
I1223 13:21:38.614311 27366 net.cpp:144] Memory required for data: 455667712
I1223 13:21:38.614331 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:1
I1223 13:21:38.614342 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:1
I1223 13:21:38.614352 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:1
I1223 13:21:38.614359 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:1
I1223 13:21:38.614365 27366 layer_factory.hpp:77] Creating layer relu1
I1223 13:21:38.614380 27366 net.cpp:84] Creating Layer relu1
I1223 13:21:38.614387 27366 net.cpp:413] relu1 <- conv1
I1223 13:21:38.614393 27366 net.cpp:374] relu1 -> conv1 (in-place)
I1223 13:21:39.146731 27366 net.cpp:127] Setting up relu1
I1223 13:21:39.146770 27366 net.cpp:136] Top shape: 256 96 55 55 (74342400)
I1223 13:21:39.146777 27366 net.cpp:144] Memory required for data: 753037312
I1223 13:21:39.146786 27366 layer_factory.hpp:77] Creating layer norm1
I1223 13:21:39.146816 27366 net.cpp:84] Creating Layer norm1
I1223 13:21:39.146822 27366 net.cpp:413] norm1 <- conv1
I1223 13:21:39.146833 27366 net.cpp:387] norm1 -> norm1
I1223 13:21:39.148325 27366 net.cpp:127] Setting up norm1
I1223 13:21:39.148344 27366 net.cpp:136] Top shape: 256 96 55 55 (74342400)
I1223 13:21:39.148349 27366 net.cpp:144] Memory required for data: 1050406912
I1223 13:21:39.148355 27366 layer_factory.hpp:77] Creating layer pool1
I1223 13:21:39.148370 27366 net.cpp:84] Creating Layer pool1
I1223 13:21:39.148375 27366 net.cpp:413] pool1 <- norm1
I1223 13:21:39.148382 27366 net.cpp:387] pool1 -> pool1
I1223 13:21:39.148439 27366 net.cpp:127] Setting up pool1
I1223 13:21:39.148448 27366 net.cpp:136] Top shape: 256 96 27 27 (17915904)
I1223 13:21:39.148453 27366 net.cpp:144] Memory required for data: 1122070528
I1223 13:21:39.148458 27366 layer_factory.hpp:77] Creating layer conv2
I1223 13:21:39.148475 27366 net.cpp:84] Creating Layer conv2
I1223 13:21:39.148480 27366 net.cpp:413] conv2 <- pool1
I1223 13:21:39.148488 27366 net.cpp:387] conv2 -> conv2
I1223 13:21:39.155448 27366 net.cpp:127] Setting up conv2
I1223 13:21:39.155467 27366 net.cpp:136] Top shape: 256 256 27 27 (47775744)
I1223 13:21:39.155472 27366 net.cpp:144] Memory required for data: 1313173504
I1223 13:21:39.155486 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:5
I1223 13:21:39.155496 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:5
I1223 13:21:39.155503 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:5
I1223 13:21:39.155509 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:5
I1223 13:21:39.155514 27366 layer_factory.hpp:77] Creating layer relu2
I1223 13:21:39.155524 27366 net.cpp:84] Creating Layer relu2
I1223 13:21:39.155530 27366 net.cpp:413] relu2 <- conv2
I1223 13:21:39.155537 27366 net.cpp:374] relu2 -> conv2 (in-place)
I1223 13:21:39.155760 27366 net.cpp:127] Setting up relu2
I1223 13:21:39.155795 27366 net.cpp:136] Top shape: 256 256 27 27 (47775744)
I1223 13:21:39.155802 27366 net.cpp:144] Memory required for data: 1504276480
I1223 13:21:39.155807 27366 layer_factory.hpp:77] Creating layer norm2
I1223 13:21:39.155817 27366 net.cpp:84] Creating Layer norm2
I1223 13:21:39.155823 27366 net.cpp:413] norm2 <- conv2
I1223 13:21:39.155829 27366 net.cpp:387] norm2 -> norm2
I1223 13:21:39.156054 27366 net.cpp:127] Setting up norm2
I1223 13:21:39.156064 27366 net.cpp:136] Top shape: 256 256 27 27 (47775744)
I1223 13:21:39.156069 27366 net.cpp:144] Memory required for data: 1695379456
I1223 13:21:39.156075 27366 layer_factory.hpp:77] Creating layer pool2
I1223 13:21:39.156085 27366 net.cpp:84] Creating Layer pool2
I1223 13:21:39.156090 27366 net.cpp:413] pool2 <- norm2
I1223 13:21:39.156096 27366 net.cpp:387] pool2 -> pool2
I1223 13:21:39.156134 27366 net.cpp:127] Setting up pool2
I1223 13:21:39.156141 27366 net.cpp:136] Top shape: 256 256 13 13 (11075584)
I1223 13:21:39.156146 27366 net.cpp:144] Memory required for data: 1739681792
I1223 13:21:39.156150 27366 layer_factory.hpp:77] Creating layer conv3
I1223 13:21:39.156163 27366 net.cpp:84] Creating Layer conv3
I1223 13:21:39.156168 27366 net.cpp:413] conv3 <- pool2
I1223 13:21:39.156175 27366 net.cpp:387] conv3 -> conv3
I1223 13:21:39.171890 27366 net.cpp:127] Setting up conv3
I1223 13:21:39.171912 27366 net.cpp:136] Top shape: 256 384 13 13 (16613376)
I1223 13:21:39.171918 27366 net.cpp:144] Memory required for data: 1806135296
I1223 13:21:39.171928 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:9
I1223 13:21:39.171938 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:9
I1223 13:21:39.171946 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:9
I1223 13:21:39.171952 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:9
I1223 13:21:39.171957 27366 layer_factory.hpp:77] Creating layer relu3
I1223 13:21:39.171965 27366 net.cpp:84] Creating Layer relu3
I1223 13:21:39.171972 27366 net.cpp:413] relu3 <- conv3
I1223 13:21:39.171978 27366 net.cpp:374] relu3 -> conv3 (in-place)
I1223 13:21:39.173362 27366 net.cpp:127] Setting up relu3
I1223 13:21:39.173379 27366 net.cpp:136] Top shape: 256 384 13 13 (16613376)
I1223 13:21:39.173384 27366 net.cpp:144] Memory required for data: 1872588800
I1223 13:21:39.173390 27366 layer_factory.hpp:77] Creating layer conv4
I1223 13:21:39.173404 27366 net.cpp:84] Creating Layer conv4
I1223 13:21:39.173409 27366 net.cpp:413] conv4 <- conv3
I1223 13:21:39.173418 27366 net.cpp:387] conv4 -> conv4
I1223 13:21:39.185497 27366 net.cpp:127] Setting up conv4
I1223 13:21:39.185516 27366 net.cpp:136] Top shape: 256 384 13 13 (16613376)
I1223 13:21:39.185523 27366 net.cpp:144] Memory required for data: 1939042304
I1223 13:21:39.185530 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:11
I1223 13:21:39.185537 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:11
I1223 13:21:39.185544 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:11
I1223 13:21:39.185549 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:11
I1223 13:21:39.185554 27366 layer_factory.hpp:77] Creating layer relu4
I1223 13:21:39.185565 27366 net.cpp:84] Creating Layer relu4
I1223 13:21:39.185571 27366 net.cpp:413] relu4 <- conv4
I1223 13:21:39.185580 27366 net.cpp:374] relu4 -> conv4 (in-place)
I1223 13:21:39.185797 27366 net.cpp:127] Setting up relu4
I1223 13:21:39.185807 27366 net.cpp:136] Top shape: 256 384 13 13 (16613376)
I1223 13:21:39.185812 27366 net.cpp:144] Memory required for data: 2005495808
I1223 13:21:39.185817 27366 layer_factory.hpp:77] Creating layer conv5
I1223 13:21:39.185832 27366 net.cpp:84] Creating Layer conv5
I1223 13:21:39.185837 27366 net.cpp:413] conv5 <- conv4
I1223 13:21:39.185847 27366 net.cpp:387] conv5 -> conv5
I1223 13:21:39.194584 27366 net.cpp:127] Setting up conv5
I1223 13:21:39.194614 27366 net.cpp:136] Top shape: 256 256 13 13 (11075584)
I1223 13:21:39.194645 27366 net.cpp:144] Memory required for data: 2049798144
I1223 13:21:39.194658 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:13
I1223 13:21:39.194667 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:13
I1223 13:21:39.194674 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:13
I1223 13:21:39.194679 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:13
I1223 13:21:39.194684 27366 layer_factory.hpp:77] Creating layer relu5
I1223 13:21:39.194694 27366 net.cpp:84] Creating Layer relu5
I1223 13:21:39.194699 27366 net.cpp:413] relu5 <- conv5
I1223 13:21:39.194706 27366 net.cpp:374] relu5 -> conv5 (in-place)
I1223 13:21:39.194928 27366 net.cpp:127] Setting up relu5
I1223 13:21:39.194938 27366 net.cpp:136] Top shape: 256 256 13 13 (11075584)
I1223 13:21:39.194943 27366 net.cpp:144] Memory required for data: 2094100480
I1223 13:21:39.194948 27366 layer_factory.hpp:77] Creating layer pool5
I1223 13:21:39.194962 27366 net.cpp:84] Creating Layer pool5
I1223 13:21:39.194967 27366 net.cpp:413] pool5 <- conv5
I1223 13:21:39.194974 27366 net.cpp:387] pool5 -> pool5
I1223 13:21:39.195025 27366 net.cpp:127] Setting up pool5
I1223 13:21:39.195034 27366 net.cpp:136] Top shape: 256 256 6 6 (2359296)
I1223 13:21:39.195037 27366 net.cpp:144] Memory required for data: 2103537664
I1223 13:21:39.195044 27366 layer_factory.hpp:77] Creating layer fc6
I1223 13:21:39.195065 27366 net.cpp:84] Creating Layer fc6
I1223 13:21:39.195070 27366 net.cpp:413] fc6 <- pool5
I1223 13:21:39.195078 27366 net.cpp:387] fc6 -> fc6
I1223 13:21:39.810247 27366 net.cpp:127] Setting up fc6
I1223 13:21:39.810308 27366 net.cpp:136] Top shape: 256 4096 (1048576)
I1223 13:21:39.810315 27366 net.cpp:144] Memory required for data: 2107731968
I1223 13:21:39.810328 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:16
I1223 13:21:39.810336 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:16
I1223 13:21:39.810343 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:16
I1223 13:21:39.810348 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:16
I1223 13:21:39.810353 27366 layer_factory.hpp:77] Creating layer relu6
I1223 13:21:39.810367 27366 net.cpp:84] Creating Layer relu6
I1223 13:21:39.810374 27366 net.cpp:413] relu6 <- fc6
I1223 13:21:39.810384 27366 net.cpp:374] relu6 -> fc6 (in-place)
I1223 13:21:39.810672 27366 net.cpp:127] Setting up relu6
I1223 13:21:39.810683 27366 net.cpp:136] Top shape: 256 4096 (1048576)
I1223 13:21:39.810688 27366 net.cpp:144] Memory required for data: 2111926272
I1223 13:21:39.810694 27366 layer_factory.hpp:77] Creating layer drop6
I1223 13:21:39.810712 27366 net.cpp:84] Creating Layer drop6
I1223 13:21:39.810719 27366 net.cpp:413] drop6 <- fc6
I1223 13:21:39.810726 27366 net.cpp:374] drop6 -> fc6 (in-place)
I1223 13:21:39.810758 27366 net.cpp:127] Setting up drop6
I1223 13:21:39.810766 27366 net.cpp:136] Top shape: 256 4096 (1048576)
I1223 13:21:39.810771 27366 net.cpp:144] Memory required for data: 2116120576
I1223 13:21:39.810776 27366 layer_factory.hpp:77] Creating layer fc7
I1223 13:21:39.810791 27366 net.cpp:84] Creating Layer fc7
I1223 13:21:39.810796 27366 net.cpp:413] fc7 <- fc6
I1223 13:21:39.810804 27366 net.cpp:387] fc7 -> fc7
I1223 13:21:40.085778 27366 net.cpp:127] Setting up fc7
I1223 13:21:40.085836 27366 net.cpp:136] Top shape: 256 4096 (1048576)
I1223 13:21:40.085842 27366 net.cpp:144] Memory required for data: 2120314880
I1223 13:21:40.085855 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:19
I1223 13:21:40.085863 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:19
I1223 13:21:40.085870 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:19
I1223 13:21:40.085875 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:19
I1223 13:21:40.085880 27366 layer_factory.hpp:77] Creating layer relu7
I1223 13:21:40.085921 27366 net.cpp:84] Creating Layer relu7
I1223 13:21:40.085953 27366 net.cpp:413] relu7 <- fc7
I1223 13:21:40.085963 27366 net.cpp:374] relu7 -> fc7 (in-place)
I1223 13:21:40.087576 27366 net.cpp:127] Setting up relu7
I1223 13:21:40.087594 27366 net.cpp:136] Top shape: 256 4096 (1048576)
I1223 13:21:40.087599 27366 net.cpp:144] Memory required for data: 2124509184
I1223 13:21:40.087605 27366 layer_factory.hpp:77] Creating layer drop7
I1223 13:21:40.087620 27366 net.cpp:84] Creating Layer drop7
I1223 13:21:40.087625 27366 net.cpp:413] drop7 <- fc7
I1223 13:21:40.087633 27366 net.cpp:374] drop7 -> fc7 (in-place)
I1223 13:21:40.087662 27366 net.cpp:127] Setting up drop7
I1223 13:21:40.087671 27366 net.cpp:136] Top shape: 256 4096 (1048576)
I1223 13:21:40.087676 27366 net.cpp:144] Memory required for data: 2128703488
I1223 13:21:40.087679 27366 layer_factory.hpp:77] Creating layer fc8
I1223 13:21:40.087693 27366 net.cpp:84] Creating Layer fc8
I1223 13:21:40.087698 27366 net.cpp:413] fc8 <- fc7
I1223 13:21:40.087707 27366 net.cpp:387] fc8 -> fc8
I1223 13:21:40.156026 27366 net.cpp:127] Setting up fc8
I1223 13:21:40.156060 27366 net.cpp:136] Top shape: 256 1000 (256000)
I1223 13:21:40.156066 27366 net.cpp:144] Memory required for data: 2129727488
I1223 13:21:40.156076 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:22
I1223 13:21:40.156085 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:22
I1223 13:21:40.156090 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:22
I1223 13:21:40.156096 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:22
I1223 13:21:40.156101 27366 layer_factory.hpp:77] Creating layer loss
I1223 13:21:40.156121 27366 net.cpp:84] Creating Layer loss
I1223 13:21:40.156128 27366 net.cpp:413] loss <- fc8
I1223 13:21:40.156136 27366 net.cpp:413] loss <- label
I1223 13:21:40.156146 27366 net.cpp:387] loss -> loss
I1223 13:21:40.156172 27366 layer_factory.hpp:77] Creating layer loss
I1223 13:21:40.158195 27366 net.cpp:127] Setting up loss
I1223 13:21:40.158211 27366 net.cpp:136] Top shape: (1)
I1223 13:21:40.158217 27366 net.cpp:139]     with loss weight 1
I1223 13:21:40.158248 27366 net.cpp:144] Memory required for data: 2129727492
I1223 13:21:40.158254 27366 net.cpp:205] loss needs backward computation.
I1223 13:21:40.158260 27366 net.cpp:205] fc8 needs backward computation.
I1223 13:21:40.158265 27366 net.cpp:205] drop7 needs backward computation.
I1223 13:21:40.158270 27366 net.cpp:205] relu7 needs backward computation.
I1223 13:21:40.158274 27366 net.cpp:205] fc7 needs backward computation.
I1223 13:21:40.158279 27366 net.cpp:205] drop6 needs backward computation.
I1223 13:21:40.158289 27366 net.cpp:205] relu6 needs backward computation.
I1223 13:21:40.158294 27366 net.cpp:205] fc6 needs backward computation.
I1223 13:21:40.158300 27366 net.cpp:205] pool5 needs backward computation.
I1223 13:21:40.158305 27366 net.cpp:205] relu5 needs backward computation.
I1223 13:21:40.158310 27366 net.cpp:205] conv5 needs backward computation.
I1223 13:21:40.158315 27366 net.cpp:205] relu4 needs backward computation.
I1223 13:21:40.158320 27366 net.cpp:205] conv4 needs backward computation.
I1223 13:21:40.158325 27366 net.cpp:205] relu3 needs backward computation.
I1223 13:21:40.158330 27366 net.cpp:205] conv3 needs backward computation.
I1223 13:21:40.158335 27366 net.cpp:205] pool2 needs backward computation.
I1223 13:21:40.158340 27366 net.cpp:205] norm2 needs backward computation.
I1223 13:21:40.158344 27366 net.cpp:205] relu2 needs backward computation.
I1223 13:21:40.158349 27366 net.cpp:205] conv2 needs backward computation.
I1223 13:21:40.158354 27366 net.cpp:205] pool1 needs backward computation.
I1223 13:21:40.158360 27366 net.cpp:205] norm1 needs backward computation.
I1223 13:21:40.158365 27366 net.cpp:205] relu1 needs backward computation.
I1223 13:21:40.158368 27366 net.cpp:205] conv1 needs backward computation.
I1223 13:21:40.158375 27366 net.cpp:207] data does not need backward computation.
I1223 13:21:40.158388 27366 net.cpp:249] This network produces output loss
I1223 13:21:40.158433 27366 net.cpp:262] Network initialization done.
I1223 13:21:40.159440 27366 solver.cpp:172] Creating test net (#0) specified by net file: models_compression/alexnet/train_val_inq90_2.prototxt
I1223 13:21:40.159499 27366 net.cpp:301] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1223 13:21:40.159878 27366 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "/home/data/caffe-imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/data/caffe-imagenet/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "INQConvolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "INQConvolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "INQConvolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "INQConvolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "INQConvolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_convolution_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "INQInnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_inner_product_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "INQInnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  inq_inner_product_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "INQInnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  inq_inner_product_param {
    portion: 0.7
    portion: 0.9
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
    num_quantum_values: 7
  }
}
layer {
  name: "accuracy_top1"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy_top1"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 1
  }
}
layer {
  name: "accuracy_top5"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy_top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1223 13:21:40.160038 27366 layer_factory.hpp:77] Creating layer data
I1223 13:21:40.160116 27366 db_lmdb.cpp:35] Opened lmdb /home/data/caffe-imagenet/ilsvrc12_val_lmdb
I1223 13:21:40.160137 27366 net.cpp:84] Creating Layer data
I1223 13:21:40.160145 27366 net.cpp:387] data -> data
I1223 13:21:40.160157 27366 net.cpp:387] data -> label
I1223 13:21:40.160167 27366 data_transformer.cpp:25] Loading mean file from: /home/data/caffe-imagenet/imagenet_mean.binaryproto
I1223 13:21:40.161998 27366 data_layer.cpp:45] output data size: 50,3,227,227
I1223 13:21:40.250190 27366 net.cpp:127] Setting up data
I1223 13:21:40.250246 27366 net.cpp:136] Top shape: 50 3 227 227 (7729350)
I1223 13:21:40.250282 27366 net.cpp:136] Top shape: 50 (50)
I1223 13:21:40.250294 27366 net.cpp:144] Memory required for data: 30917600
I1223 13:21:40.250303 27366 layer_factory.hpp:77] Creating layer label_data_1_split
I1223 13:21:40.250326 27366 net.cpp:84] Creating Layer label_data_1_split
I1223 13:21:40.250332 27366 net.cpp:413] label_data_1_split <- label
I1223 13:21:40.250342 27366 net.cpp:387] label_data_1_split -> label_data_1_split_0
I1223 13:21:40.250355 27366 net.cpp:387] label_data_1_split -> label_data_1_split_1
I1223 13:21:40.250363 27366 net.cpp:387] label_data_1_split -> label_data_1_split_2
I1223 13:21:40.250494 27366 net.cpp:127] Setting up label_data_1_split
I1223 13:21:40.250504 27366 net.cpp:136] Top shape: 50 (50)
I1223 13:21:40.250509 27366 net.cpp:136] Top shape: 50 (50)
I1223 13:21:40.250514 27366 net.cpp:136] Top shape: 50 (50)
I1223 13:21:40.250519 27366 net.cpp:144] Memory required for data: 30918200
I1223 13:21:40.250524 27366 layer_factory.hpp:77] Creating layer conv1
I1223 13:21:40.250540 27366 net.cpp:84] Creating Layer conv1
I1223 13:21:40.250545 27366 net.cpp:413] conv1 <- data
I1223 13:21:40.250555 27366 net.cpp:387] conv1 -> conv1
I1223 13:21:40.251477 27366 net.cpp:127] Setting up conv1
I1223 13:21:40.251488 27366 net.cpp:136] Top shape: 50 96 55 55 (14520000)
I1223 13:21:40.251493 27366 net.cpp:144] Memory required for data: 88998200
I1223 13:21:40.251502 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:2
I1223 13:21:40.251513 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:2
I1223 13:21:40.251521 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:2
I1223 13:21:40.251528 27366 net.cpp:453] Found INQ layer:conv1, type: INQConvolution, layer id:2
I1223 13:21:40.251533 27366 layer_factory.hpp:77] Creating layer relu1
I1223 13:21:40.251543 27366 net.cpp:84] Creating Layer relu1
I1223 13:21:40.251549 27366 net.cpp:413] relu1 <- conv1
I1223 13:21:40.251555 27366 net.cpp:374] relu1 -> conv1 (in-place)
I1223 13:21:40.251813 27366 net.cpp:127] Setting up relu1
I1223 13:21:40.251823 27366 net.cpp:136] Top shape: 50 96 55 55 (14520000)
I1223 13:21:40.251828 27366 net.cpp:144] Memory required for data: 147078200
I1223 13:21:40.251833 27366 layer_factory.hpp:77] Creating layer norm1
I1223 13:21:40.251844 27366 net.cpp:84] Creating Layer norm1
I1223 13:21:40.251849 27366 net.cpp:413] norm1 <- conv1
I1223 13:21:40.251857 27366 net.cpp:387] norm1 -> norm1
I1223 13:21:40.252102 27366 net.cpp:127] Setting up norm1
I1223 13:21:40.252112 27366 net.cpp:136] Top shape: 50 96 55 55 (14520000)
I1223 13:21:40.252117 27366 net.cpp:144] Memory required for data: 205158200
I1223 13:21:40.252122 27366 layer_factory.hpp:77] Creating layer pool1
I1223 13:21:40.252131 27366 net.cpp:84] Creating Layer pool1
I1223 13:21:40.252136 27366 net.cpp:413] pool1 <- norm1
I1223 13:21:40.252144 27366 net.cpp:387] pool1 -> pool1
I1223 13:21:40.252190 27366 net.cpp:127] Setting up pool1
I1223 13:21:40.252198 27366 net.cpp:136] Top shape: 50 96 27 27 (3499200)
I1223 13:21:40.252203 27366 net.cpp:144] Memory required for data: 219155000
I1223 13:21:40.252208 27366 layer_factory.hpp:77] Creating layer conv2
I1223 13:21:40.252219 27366 net.cpp:84] Creating Layer conv2
I1223 13:21:40.252224 27366 net.cpp:413] conv2 <- pool1
I1223 13:21:40.252233 27366 net.cpp:387] conv2 -> conv2
I1223 13:21:40.259235 27366 net.cpp:127] Setting up conv2
I1223 13:21:40.259254 27366 net.cpp:136] Top shape: 50 256 27 27 (9331200)
I1223 13:21:40.259259 27366 net.cpp:144] Memory required for data: 256479800
I1223 13:21:40.259269 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:6
I1223 13:21:40.259279 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:6
I1223 13:21:40.259292 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:6
I1223 13:21:40.259299 27366 net.cpp:453] Found INQ layer:conv2, type: INQConvolution, layer id:6
I1223 13:21:40.259310 27366 layer_factory.hpp:77] Creating layer relu2
I1223 13:21:40.259331 27366 net.cpp:84] Creating Layer relu2
I1223 13:21:40.259337 27366 net.cpp:413] relu2 <- conv2
I1223 13:21:40.259344 27366 net.cpp:374] relu2 -> conv2 (in-place)
I1223 13:21:40.260757 27366 net.cpp:127] Setting up relu2
I1223 13:21:40.260774 27366 net.cpp:136] Top shape: 50 256 27 27 (9331200)
I1223 13:21:40.260781 27366 net.cpp:144] Memory required for data: 293804600
I1223 13:21:40.260785 27366 layer_factory.hpp:77] Creating layer norm2
I1223 13:21:40.260797 27366 net.cpp:84] Creating Layer norm2
I1223 13:21:40.260802 27366 net.cpp:413] norm2 <- conv2
I1223 13:21:40.260810 27366 net.cpp:387] norm2 -> norm2
I1223 13:21:40.261045 27366 net.cpp:127] Setting up norm2
I1223 13:21:40.261055 27366 net.cpp:136] Top shape: 50 256 27 27 (9331200)
I1223 13:21:40.261060 27366 net.cpp:144] Memory required for data: 331129400
I1223 13:21:40.261065 27366 layer_factory.hpp:77] Creating layer pool2
I1223 13:21:40.261075 27366 net.cpp:84] Creating Layer pool2
I1223 13:21:40.261080 27366 net.cpp:413] pool2 <- norm2
I1223 13:21:40.261086 27366 net.cpp:387] pool2 -> pool2
I1223 13:21:40.261128 27366 net.cpp:127] Setting up pool2
I1223 13:21:40.261135 27366 net.cpp:136] Top shape: 50 256 13 13 (2163200)
I1223 13:21:40.261140 27366 net.cpp:144] Memory required for data: 339782200
I1223 13:21:40.261145 27366 layer_factory.hpp:77] Creating layer conv3
I1223 13:21:40.261157 27366 net.cpp:84] Creating Layer conv3
I1223 13:21:40.261162 27366 net.cpp:413] conv3 <- pool2
I1223 13:21:40.261170 27366 net.cpp:387] conv3 -> conv3
I1223 13:21:40.276986 27366 net.cpp:127] Setting up conv3
I1223 13:21:40.277004 27366 net.cpp:136] Top shape: 50 384 13 13 (3244800)
I1223 13:21:40.277010 27366 net.cpp:144] Memory required for data: 352761400
I1223 13:21:40.277020 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:10
I1223 13:21:40.277029 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:10
I1223 13:21:40.277035 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:10
I1223 13:21:40.277041 27366 net.cpp:453] Found INQ layer:conv3, type: INQConvolution, layer id:10
I1223 13:21:40.277046 27366 layer_factory.hpp:77] Creating layer relu3
I1223 13:21:40.277055 27366 net.cpp:84] Creating Layer relu3
I1223 13:21:40.277060 27366 net.cpp:413] relu3 <- conv3
I1223 13:21:40.277068 27366 net.cpp:374] relu3 -> conv3 (in-place)
I1223 13:21:40.277295 27366 net.cpp:127] Setting up relu3
I1223 13:21:40.277307 27366 net.cpp:136] Top shape: 50 384 13 13 (3244800)
I1223 13:21:40.277312 27366 net.cpp:144] Memory required for data: 365740600
I1223 13:21:40.277317 27366 layer_factory.hpp:77] Creating layer conv4
I1223 13:21:40.277328 27366 net.cpp:84] Creating Layer conv4
I1223 13:21:40.277333 27366 net.cpp:413] conv4 <- conv3
I1223 13:21:40.277343 27366 net.cpp:387] conv4 -> conv4
I1223 13:21:40.289486 27366 net.cpp:127] Setting up conv4
I1223 13:21:40.289505 27366 net.cpp:136] Top shape: 50 384 13 13 (3244800)
I1223 13:21:40.289511 27366 net.cpp:144] Memory required for data: 378719800
I1223 13:21:40.289518 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:12
I1223 13:21:40.289525 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:12
I1223 13:21:40.289531 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:12
I1223 13:21:40.289536 27366 net.cpp:453] Found INQ layer:conv4, type: INQConvolution, layer id:12
I1223 13:21:40.289541 27366 layer_factory.hpp:77] Creating layer relu4
I1223 13:21:40.289549 27366 net.cpp:84] Creating Layer relu4
I1223 13:21:40.289554 27366 net.cpp:413] relu4 <- conv4
I1223 13:21:40.289562 27366 net.cpp:374] relu4 -> conv4 (in-place)
I1223 13:21:40.290998 27366 net.cpp:127] Setting up relu4
I1223 13:21:40.291015 27366 net.cpp:136] Top shape: 50 384 13 13 (3244800)
I1223 13:21:40.291020 27366 net.cpp:144] Memory required for data: 391699000
I1223 13:21:40.291026 27366 layer_factory.hpp:77] Creating layer conv5
I1223 13:21:40.291038 27366 net.cpp:84] Creating Layer conv5
I1223 13:21:40.291050 27366 net.cpp:413] conv5 <- conv4
I1223 13:21:40.291072 27366 net.cpp:387] conv5 -> conv5
I1223 13:21:40.299932 27366 net.cpp:127] Setting up conv5
I1223 13:21:40.299952 27366 net.cpp:136] Top shape: 50 256 13 13 (2163200)
I1223 13:21:40.299957 27366 net.cpp:144] Memory required for data: 400351800
I1223 13:21:40.299968 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:14
I1223 13:21:40.299978 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:14
I1223 13:21:40.299984 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:14
I1223 13:21:40.299990 27366 net.cpp:453] Found INQ layer:conv5, type: INQConvolution, layer id:14
I1223 13:21:40.299995 27366 layer_factory.hpp:77] Creating layer relu5
I1223 13:21:40.300004 27366 net.cpp:84] Creating Layer relu5
I1223 13:21:40.300009 27366 net.cpp:413] relu5 <- conv5
I1223 13:21:40.300016 27366 net.cpp:374] relu5 -> conv5 (in-place)
I1223 13:21:40.300237 27366 net.cpp:127] Setting up relu5
I1223 13:21:40.300248 27366 net.cpp:136] Top shape: 50 256 13 13 (2163200)
I1223 13:21:40.300253 27366 net.cpp:144] Memory required for data: 409004600
I1223 13:21:40.300258 27366 layer_factory.hpp:77] Creating layer pool5
I1223 13:21:40.300269 27366 net.cpp:84] Creating Layer pool5
I1223 13:21:40.300274 27366 net.cpp:413] pool5 <- conv5
I1223 13:21:40.300282 27366 net.cpp:387] pool5 -> pool5
I1223 13:21:40.300343 27366 net.cpp:127] Setting up pool5
I1223 13:21:40.300349 27366 net.cpp:136] Top shape: 50 256 6 6 (460800)
I1223 13:21:40.300354 27366 net.cpp:144] Memory required for data: 410847800
I1223 13:21:40.300359 27366 layer_factory.hpp:77] Creating layer fc6
I1223 13:21:40.300371 27366 net.cpp:84] Creating Layer fc6
I1223 13:21:40.300376 27366 net.cpp:413] fc6 <- pool5
I1223 13:21:40.300384 27366 net.cpp:387] fc6 -> fc6
I1223 13:21:40.915581 27366 net.cpp:127] Setting up fc6
I1223 13:21:40.915634 27366 net.cpp:136] Top shape: 50 4096 (204800)
I1223 13:21:40.915640 27366 net.cpp:144] Memory required for data: 411667000
I1223 13:21:40.915652 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:17
I1223 13:21:40.915662 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:17
I1223 13:21:40.915669 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:17
I1223 13:21:40.915674 27366 net.cpp:453] Found INQ layer:fc6, type: INQInnerProduct, layer id:17
I1223 13:21:40.915679 27366 layer_factory.hpp:77] Creating layer relu6
I1223 13:21:40.915693 27366 net.cpp:84] Creating Layer relu6
I1223 13:21:40.915700 27366 net.cpp:413] relu6 <- fc6
I1223 13:21:40.915709 27366 net.cpp:374] relu6 -> fc6 (in-place)
I1223 13:21:40.915999 27366 net.cpp:127] Setting up relu6
I1223 13:21:40.916010 27366 net.cpp:136] Top shape: 50 4096 (204800)
I1223 13:21:40.916015 27366 net.cpp:144] Memory required for data: 412486200
I1223 13:21:40.916020 27366 layer_factory.hpp:77] Creating layer drop6
I1223 13:21:40.916030 27366 net.cpp:84] Creating Layer drop6
I1223 13:21:40.916035 27366 net.cpp:413] drop6 <- fc6
I1223 13:21:40.916043 27366 net.cpp:374] drop6 -> fc6 (in-place)
I1223 13:21:40.916079 27366 net.cpp:127] Setting up drop6
I1223 13:21:40.916086 27366 net.cpp:136] Top shape: 50 4096 (204800)
I1223 13:21:40.916091 27366 net.cpp:144] Memory required for data: 413305400
I1223 13:21:40.916096 27366 layer_factory.hpp:77] Creating layer fc7
I1223 13:21:40.916110 27366 net.cpp:84] Creating Layer fc7
I1223 13:21:40.916115 27366 net.cpp:413] fc7 <- fc6
I1223 13:21:40.916123 27366 net.cpp:387] fc7 -> fc7
I1223 13:21:41.193120 27366 net.cpp:127] Setting up fc7
I1223 13:21:41.193174 27366 net.cpp:136] Top shape: 50 4096 (204800)
I1223 13:21:41.193181 27366 net.cpp:144] Memory required for data: 414124600
I1223 13:21:41.193195 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:20
I1223 13:21:41.193203 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:20
I1223 13:21:41.193210 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:20
I1223 13:21:41.193231 27366 net.cpp:453] Found INQ layer:fc7, type: INQInnerProduct, layer id:20
I1223 13:21:41.193264 27366 layer_factory.hpp:77] Creating layer relu7
I1223 13:21:41.193279 27366 net.cpp:84] Creating Layer relu7
I1223 13:21:41.193295 27366 net.cpp:413] relu7 <- fc7
I1223 13:21:41.193306 27366 net.cpp:374] relu7 -> fc7 (in-place)
I1223 13:21:41.193706 27366 net.cpp:127] Setting up relu7
I1223 13:21:41.193717 27366 net.cpp:136] Top shape: 50 4096 (204800)
I1223 13:21:41.193722 27366 net.cpp:144] Memory required for data: 414943800
I1223 13:21:41.193727 27366 layer_factory.hpp:77] Creating layer drop7
I1223 13:21:41.193738 27366 net.cpp:84] Creating Layer drop7
I1223 13:21:41.193743 27366 net.cpp:413] drop7 <- fc7
I1223 13:21:41.193753 27366 net.cpp:374] drop7 -> fc7 (in-place)
I1223 13:21:41.193792 27366 net.cpp:127] Setting up drop7
I1223 13:21:41.193799 27366 net.cpp:136] Top shape: 50 4096 (204800)
I1223 13:21:41.193804 27366 net.cpp:144] Memory required for data: 415763000
I1223 13:21:41.193809 27366 layer_factory.hpp:77] Creating layer fc8
I1223 13:21:41.193823 27366 net.cpp:84] Creating Layer fc8
I1223 13:21:41.193828 27366 net.cpp:413] fc8 <- fc7
I1223 13:21:41.193838 27366 net.cpp:387] fc8 -> fc8
I1223 13:21:41.262452 27366 net.cpp:127] Setting up fc8
I1223 13:21:41.262488 27366 net.cpp:136] Top shape: 50 1000 (50000)
I1223 13:21:41.262495 27366 net.cpp:144] Memory required for data: 415963000
I1223 13:21:41.262503 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:23
I1223 13:21:41.262511 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:23
I1223 13:21:41.262518 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:23
I1223 13:21:41.262523 27366 net.cpp:453] Found INQ layer:fc8, type: INQInnerProduct, layer id:23
I1223 13:21:41.262528 27366 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1223 13:21:41.262542 27366 net.cpp:84] Creating Layer fc8_fc8_0_split
I1223 13:21:41.262549 27366 net.cpp:413] fc8_fc8_0_split <- fc8
I1223 13:21:41.262558 27366 net.cpp:387] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1223 13:21:41.262570 27366 net.cpp:387] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1223 13:21:41.262578 27366 net.cpp:387] fc8_fc8_0_split -> fc8_fc8_0_split_2
I1223 13:21:41.262645 27366 net.cpp:127] Setting up fc8_fc8_0_split
I1223 13:21:41.262653 27366 net.cpp:136] Top shape: 50 1000 (50000)
I1223 13:21:41.262660 27366 net.cpp:136] Top shape: 50 1000 (50000)
I1223 13:21:41.262665 27366 net.cpp:136] Top shape: 50 1000 (50000)
I1223 13:21:41.262668 27366 net.cpp:144] Memory required for data: 416563000
I1223 13:21:41.262673 27366 layer_factory.hpp:77] Creating layer accuracy_top1
I1223 13:21:41.262691 27366 net.cpp:84] Creating Layer accuracy_top1
I1223 13:21:41.262696 27366 net.cpp:413] accuracy_top1 <- fc8_fc8_0_split_0
I1223 13:21:41.262702 27366 net.cpp:413] accuracy_top1 <- label_data_1_split_0
I1223 13:21:41.262711 27366 net.cpp:387] accuracy_top1 -> accuracy_top1
I1223 13:21:41.262727 27366 net.cpp:127] Setting up accuracy_top1
I1223 13:21:41.262734 27366 net.cpp:136] Top shape: (1)
I1223 13:21:41.262738 27366 net.cpp:144] Memory required for data: 416563004
I1223 13:21:41.262744 27366 layer_factory.hpp:77] Creating layer accuracy_top5
I1223 13:21:41.262753 27366 net.cpp:84] Creating Layer accuracy_top5
I1223 13:21:41.262758 27366 net.cpp:413] accuracy_top5 <- fc8_fc8_0_split_1
I1223 13:21:41.262763 27366 net.cpp:413] accuracy_top5 <- label_data_1_split_1
I1223 13:21:41.262770 27366 net.cpp:387] accuracy_top5 -> accuracy_top5
I1223 13:21:41.262778 27366 net.cpp:127] Setting up accuracy_top5
I1223 13:21:41.262784 27366 net.cpp:136] Top shape: (1)
I1223 13:21:41.262789 27366 net.cpp:144] Memory required for data: 416563008
I1223 13:21:41.262794 27366 layer_factory.hpp:77] Creating layer loss
I1223 13:21:41.262801 27366 net.cpp:84] Creating Layer loss
I1223 13:21:41.262806 27366 net.cpp:413] loss <- fc8_fc8_0_split_2
I1223 13:21:41.262812 27366 net.cpp:413] loss <- label_data_1_split_2
I1223 13:21:41.262818 27366 net.cpp:387] loss -> loss
I1223 13:21:41.262838 27366 layer_factory.hpp:77] Creating layer loss
I1223 13:21:41.264606 27366 net.cpp:127] Setting up loss
I1223 13:21:41.264624 27366 net.cpp:136] Top shape: (1)
I1223 13:21:41.264629 27366 net.cpp:139]     with loss weight 1
I1223 13:21:41.264643 27366 net.cpp:144] Memory required for data: 416563012
I1223 13:21:41.264648 27366 net.cpp:205] loss needs backward computation.
I1223 13:21:41.264654 27366 net.cpp:207] accuracy_top5 does not need backward computation.
I1223 13:21:41.264660 27366 net.cpp:207] accuracy_top1 does not need backward computation.
I1223 13:21:41.264667 27366 net.cpp:205] fc8_fc8_0_split needs backward computation.
I1223 13:21:41.264672 27366 net.cpp:205] fc8 needs backward computation.
I1223 13:21:41.264675 27366 net.cpp:205] drop7 needs backward computation.
I1223 13:21:41.264680 27366 net.cpp:205] relu7 needs backward computation.
I1223 13:21:41.264684 27366 net.cpp:205] fc7 needs backward computation.
I1223 13:21:41.264689 27366 net.cpp:205] drop6 needs backward computation.
I1223 13:21:41.264693 27366 net.cpp:205] relu6 needs backward computation.
I1223 13:21:41.264699 27366 net.cpp:205] fc6 needs backward computation.
I1223 13:21:41.264704 27366 net.cpp:205] pool5 needs backward computation.
I1223 13:21:41.264709 27366 net.cpp:205] relu5 needs backward computation.
I1223 13:21:41.264714 27366 net.cpp:205] conv5 needs backward computation.
I1223 13:21:41.264719 27366 net.cpp:205] relu4 needs backward computation.
I1223 13:21:41.264725 27366 net.cpp:205] conv4 needs backward computation.
I1223 13:21:41.264730 27366 net.cpp:205] relu3 needs backward computation.
I1223 13:21:41.264735 27366 net.cpp:205] conv3 needs backward computation.
I1223 13:21:41.264741 27366 net.cpp:205] pool2 needs backward computation.
I1223 13:21:41.264746 27366 net.cpp:205] norm2 needs backward computation.
I1223 13:21:41.264751 27366 net.cpp:205] relu2 needs backward computation.
I1223 13:21:41.264757 27366 net.cpp:205] conv2 needs backward computation.
I1223 13:21:41.264763 27366 net.cpp:205] pool1 needs backward computation.
I1223 13:21:41.264770 27366 net.cpp:205] norm1 needs backward computation.
I1223 13:21:41.264773 27366 net.cpp:205] relu1 needs backward computation.
I1223 13:21:41.264780 27366 net.cpp:205] conv1 needs backward computation.
I1223 13:21:41.264786 27366 net.cpp:207] label_data_1_split does not need backward computation.
I1223 13:21:41.264791 27366 net.cpp:207] data does not need backward computation.
I1223 13:21:41.264796 27366 net.cpp:249] This network produces output accuracy_top1
I1223 13:21:41.264801 27366 net.cpp:249] This network produces output accuracy_top5
I1223 13:21:41.264806 27366 net.cpp:249] This network produces output loss
I1223 13:21:41.264829 27366 net.cpp:262] Network initialization done.
I1223 13:21:41.264973 27366 solver.cpp:56] Solver scaffolding done.
I1223 13:21:41.266414 27366 caffe.cpp:155] Finetuning from models_compression/alexnet/ref_inq90_82000.caffemodel
I1223 13:21:43.698019 27366 caffe.cpp:248] Starting Optimization
I1223 13:21:52.517581 27533 solver.cpp:172] Creating test net (#0) specified by net file: models_compression/alexnet/train_val_inq90_2.prototxt
I1223 13:21:52.766651 27532 solver.cpp:172] Creating test net (#0) specified by net file: models_compression/alexnet/train_val_inq90_2.prototxt
I1223 13:21:52.882721 27534 solver.cpp:172] Creating test net (#0) specified by net file: models_compression/alexnet/train_val_inq90_2.prototxt
I1223 13:21:54.855306 27366 solver.cpp:276] Solving AlexNet
I1223 13:21:54.855376 27366 solver.cpp:277] Learning Rate Policy: exp
I1223 13:21:54.855548 27366 solver.cpp:334] Iteration 0, Testing net (#0)
I1223 13:22:07.397742 27366 solver.cpp:401]     Test net output #0: accuracy_top1 = 0.57384
I1223 13:22:07.397799 27366 solver.cpp:401]     Test net output #1: accuracy_top5 = 0.80232
I1223 13:22:07.397814 27366 solver.cpp:401]     Test net output #2: loss = 1.85871 (* 1 = 1.85871 loss)
I1223 13:22:07.397877 27366 inq_conv_layer.cu:52] conv1 (INQConvolution):  Shaping the weights...
I1223 13:22:07.398154 27366 inq_conv_layer.cpp:263] Max_power = -1
I1223 13:22:07.398166 27366 inq_conv_layer.cpp:264] Min_power = -7
I1223 13:22:07.398440 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 74.3601% -> 91.4543%)
I1223 13:22:07.398460 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 29783/34848
I1223 13:22:07.398465 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 5957/2978/8935
I1223 13:22:07.400158 27366 inq_conv_layer.cu:62] conv1 (INQConvolution):  Shaping the bias...
I1223 13:22:07.400204 27366 inq_conv_layer.cpp:263] Max_power = 0
I1223 13:22:07.400214 27366 inq_conv_layer.cpp:264] Min_power = -6
I1223 13:22:07.400223 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 69.7917% -> 89.5833%)
I1223 13:22:07.400236 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 97/96
I1223 13:22:07.400241 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 19/10/29
I1223 13:22:07.416795 27366 inq_conv_layer.cu:52] conv2 (INQConvolution):  Shaping the weights...
I1223 13:22:07.445546 27366 inq_conv_layer.cpp:263] Max_power = -1
I1223 13:22:07.445561 27366 inq_conv_layer.cpp:264] Min_power = -7
I1223 13:22:07.446684 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 86.9408% -> 95.6468%)
I1223 13:22:07.446703 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 133727/307200
I1223 13:22:07.446709 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 26745/13373/40118
I1223 13:22:07.454885 27366 inq_conv_layer.cu:62] conv2 (INQConvolution):  Shaping the bias...
I1223 13:22:07.454929 27366 inq_conv_layer.cpp:263] Max_power = -2
I1223 13:22:07.454937 27366 inq_conv_layer.cpp:264] Min_power = -8
I1223 13:22:07.454948 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 69.9219% -> 89.8438%)
I1223 13:22:07.454962 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 257/256
I1223 13:22:07.454967 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 51/26/77
I1223 13:22:07.475543 27366 inq_conv_layer.cu:52] conv3 (INQConvolution):  Shaping the weights...
I1223 13:22:07.524437 27366 inq_conv_layer.cpp:263] Max_power = -1
I1223 13:22:07.524610 27366 inq_conv_layer.cpp:264] Min_power = -7
I1223 13:22:07.526940 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 90.8314% -> 96.9438%)
I1223 13:22:07.526960 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 270393/884736
I1223 13:22:07.526967 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 54079/27039/81118
I1223 13:22:07.544204 27366 inq_conv_layer.cu:62] conv3 (INQConvolution):  Shaping the bias...
I1223 13:22:07.544250 27366 inq_conv_layer.cpp:263] Max_power = -3
I1223 13:22:07.544260 27366 inq_conv_layer.cpp:264] Min_power = -9
I1223 13:22:07.544271 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 83.0729% -> 94.2708%)
I1223 13:22:07.544289 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 217/384
I1223 13:22:07.544298 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 43/22/65
I1223 13:22:07.554085 27366 inq_conv_layer.cu:52] conv4 (INQConvolution):  Shaping the weights...
I1223 13:22:07.579756 27366 inq_conv_layer.cpp:263] Max_power = -2
I1223 13:22:07.579769 27366 inq_conv_layer.cpp:264] Min_power = -8
I1223 13:22:07.581426 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 89.8214% -> 96.6072%)
I1223 13:22:07.581444 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 225133/663552
I1223 13:22:07.581450 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 45027/22513/67540
I1223 13:22:07.595917 27366 inq_conv_layer.cu:62] conv4 (INQConvolution):  Shaping the bias...
I1223 13:22:07.595960 27366 inq_conv_layer.cpp:263] Max_power = -1
I1223 13:22:07.595970 27366 inq_conv_layer.cpp:264] Min_power = -7
I1223 13:22:07.595983 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 71.3542% -> 90.3646%)
I1223 13:22:07.595995 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 367/384
I1223 13:22:07.596000 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 73/37/110
I1223 13:22:07.610678 27366 inq_conv_layer.cu:52] conv5 (INQConvolution):  Shaping the weights...
I1223 13:22:07.646181 27366 inq_conv_layer.cpp:263] Max_power = -2
I1223 13:22:07.646195 27366 inq_conv_layer.cpp:264] Min_power = -8
I1223 13:22:07.647270 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 89.9764% -> 96.6589%)
I1223 13:22:07.647292 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 147803/442368
I1223 13:22:07.647298 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 29561/14780/44341
I1223 13:22:07.656703 27366 inq_conv_layer.cu:62] conv5 (INQConvolution):  Shaping the bias...
I1223 13:22:07.656746 27366 inq_conv_layer.cpp:263] Max_power = 1
I1223 13:22:07.656756 27366 inq_conv_layer.cpp:264] Min_power = -5
I1223 13:22:07.656767 27366 inq_conv_layer.cpp:307] portions: 70% -> 90% (total: 71.0938% -> 90.2344%)
I1223 13:22:07.656780 27366 inq_conv_layer.cpp:313] init_not_quantized/total: 247/256
I1223 13:22:07.656786 27366 inq_conv_layer.cpp:316] to_update/not_tobe_quantized/not_yet_quantized: 49/25/74
I1223 13:22:07.675082 27366 inq_inner_product_layer.cu:119] fc6 (INQInnerProduct):  Shaping the weights...
I1223 13:22:07.882149 27366 inq_inner_product_layer.cpp:276] Max_power = -4
I1223 13:22:07.882194 27366 inq_inner_product_layer.cpp:277] Min_power = -10
I1223 13:22:07.940976 27366 inq_inner_product_layer.cpp:321] portions: 70% -> 90% (total: 99.0356% -> 99.6785%)
I1223 13:22:07.941031 27366 inq_inner_product_layer.cpp:327] init_not_quantized/total: 1213437/37748736
I1223 13:22:07.941038 27366 inq_inner_product_layer.cpp:329] to_update/not_tobe_quantized/not_yet_quantized: 242687/121344/364031
I1223 13:22:08.075577 27366 inq_inner_product_layer.cu:129] fc6 (INQInnerProduct):  Shaping the bias...
I1223 13:22:08.075721 27366 inq_inner_product_layer.cpp:276] Max_power = -2
I1223 13:22:08.075731 27366 inq_inner_product_layer.cpp:277] Min_power = -8
I1223 13:22:08.075773 27366 inq_inner_product_layer.cpp:321] portions: 70% -> 90% (total: 70.3125% -> 90.1123%)
I1223 13:22:08.075798 27366 inq_inner_product_layer.cpp:327] init_not_quantized/total: 4053/4096
I1223 13:22:08.075839 27366 inq_inner_product_layer.cpp:329] to_update/not_tobe_quantized/not_yet_quantized: 811/405/1216
I1223 13:22:08.102713 27366 inq_inner_product_layer.cu:119] fc7 (INQInnerProduct):  Shaping the weights...
I1223 13:22:08.181586 27366 inq_inner_product_layer.cpp:276] Max_power = -3
I1223 13:22:08.181632 27366 inq_inner_product_layer.cpp:277] Min_power = -9
I1223 13:22:08.209839 27366 inq_inner_product_layer.cpp:321] portions: 70% -> 90% (total: 98.7798% -> 99.5933%)
I1223 13:22:08.209882 27366 inq_inner_product_layer.cpp:327] init_not_quantized/total: 682390/16777216
I1223 13:22:08.209887 27366 inq_inner_product_layer.cpp:329] to_update/not_tobe_quantized/not_yet_quantized: 136478/68239/204717
I1223 13:22:08.281774 27366 inq_inner_product_layer.cu:129] fc7 (INQInnerProduct):  Shaping the bias...
I1223 13:22:08.281913 27366 inq_inner_product_layer.cpp:276] Max_power = 0
I1223 13:22:08.281925 27366 inq_inner_product_layer.cpp:277] Min_power = -6
I1223 13:22:08.281967 27366 inq_inner_product_layer.cpp:321] portions: 70% -> 90% (total: 70.0439% -> 90.0146%)
I1223 13:22:08.281992 27366 inq_inner_product_layer.cpp:327] init_not_quantized/total: 4090/4096
I1223 13:22:08.281999 27366 inq_inner_product_layer.cpp:329] to_update/not_tobe_quantized/not_yet_quantized: 818/409/1227
I1223 13:22:08.293910 27366 inq_inner_product_layer.cu:119] fc8 (INQInnerProduct):  Shaping the weights...
I1223 13:22:08.313170 27366 inq_inner_product_layer.cpp:276] Max_power = -3
I1223 13:22:08.313187 27366 inq_inner_product_layer.cpp:277] Min_power = -9
I1223 13:22:08.319459 27366 inq_inner_product_layer.cpp:321] portions: 70% -> 90% (total: 98.5076% -> 99.5025%)
I1223 13:22:08.319485 27366 inq_inner_product_layer.cpp:327] init_not_quantized/total: 203763/4096000
I1223 13:22:08.319491 27366 inq_inner_product_layer.cpp:329] to_update/not_tobe_quantized/not_yet_quantized: 40753/20376/61129
I1223 13:22:08.336725 27366 inq_inner_product_layer.cu:129] fc8 (INQInnerProduct):  Shaping the bias...
I1223 13:22:08.336793 27366 inq_inner_product_layer.cpp:276] Max_power = -1
I1223 13:22:08.336804 27366 inq_inner_product_layer.cpp:277] Min_power = -7
I1223 13:22:08.336820 27366 inq_inner_product_layer.cpp:321] portions: 70% -> 90% (total: 73.9% -> 91.3%)
I1223 13:22:08.336834 27366 inq_inner_product_layer.cpp:327] init_not_quantized/total: 870/1000
I1223 13:22:08.336841 27366 inq_inner_product_layer.cpp:329] to_update/not_tobe_quantized/not_yet_quantized: 174/87/261
I1223 13:22:08.836087 27366 solver.cpp:222] Iteration 0 (34.6226 iter/s, 13.9213s/25 iters), loss = 1.95683
I1223 13:22:08.836135 27366 solver.cpp:241]     Train net output #0: loss = 1.95683 (* 1 = 1.95683 loss)
I1223 13:22:08.836194 27366 sgd_solver.cpp:119] Iteration 0, lr = 5e-06
I1223 13:22:24.823963 27366 solver.cpp:222] Iteration 25 (1.56375 iter/s, 15.9872s/25 iters), loss = 1.89902
I1223 13:22:24.824034 27366 solver.cpp:241]     Train net output #0: loss = 1.89902 (* 1 = 1.89902 loss)
I1223 13:22:24.824051 27366 sgd_solver.cpp:119] Iteration 25, lr = 0.000129703
I1223 13:22:40.772480 27366 solver.cpp:222] Iteration 50 (1.56761 iter/s, 15.9478s/25 iters), loss = 1.89982
I1223 13:22:40.772732 27366 solver.cpp:241]     Train net output #0: loss = 1.89982 (* 1 = 1.89982 loss)
I1223 13:22:40.772754 27366 sgd_solver.cpp:119] Iteration 50, lr = 0.000254061
I1223 13:22:56.823947 27366 solver.cpp:222] Iteration 75 (1.55757 iter/s, 16.0506s/25 iters), loss = 1.66079
I1223 13:22:56.824015 27366 solver.cpp:241]     Train net output #0: loss = 1.66079 (* 1 = 1.66079 loss)
I1223 13:22:56.824033 27366 sgd_solver.cpp:119] Iteration 75, lr = 0.000378076
I1223 13:23:12.777797 27366 solver.cpp:222] Iteration 100 (1.56709 iter/s, 15.9532s/25 iters), loss = 1.76204
I1223 13:23:12.778038 27366 solver.cpp:241]     Train net output #0: loss = 1.76204 (* 1 = 1.76204 loss)
I1223 13:23:12.778064 27366 sgd_solver.cpp:119] Iteration 100, lr = 0.000501748
I1223 13:23:28.744683 27366 solver.cpp:222] Iteration 125 (1.56582 iter/s, 15.966s/25 iters), loss = 1.83752
I1223 13:23:28.744732 27366 solver.cpp:241]     Train net output #0: loss = 1.83752 (* 1 = 1.83752 loss)
I1223 13:23:28.744761 27366 sgd_solver.cpp:119] Iteration 125, lr = 0.000625078
I1223 13:23:35.124083 27366 blocking_queue.cpp:49] Waiting for data
I1223 13:23:50.545176 27366 solver.cpp:222] Iteration 150 (1.14681 iter/s, 21.7996s/25 iters), loss = 1.77054
I1223 13:23:50.545385 27366 solver.cpp:241]     Train net output #0: loss = 1.77054 (* 1 = 1.77054 loss)
I1223 13:23:50.545414 27366 sgd_solver.cpp:119] Iteration 150, lr = 0.000748066
I1223 13:24:15.674693 27366 solver.cpp:222] Iteration 175 (0.994892 iter/s, 25.1284s/25 iters), loss = 1.8145
I1223 13:24:15.674762 27366 solver.cpp:241]     Train net output #0: loss = 1.8145 (* 1 = 1.8145 loss)
I1223 13:24:15.674778 27366 sgd_solver.cpp:119] Iteration 175, lr = 0.000870714
I1223 13:24:34.243698 27366 solver.cpp:222] Iteration 200 (1.34639 iter/s, 18.5682s/25 iters), loss = 1.8691
I1223 13:24:34.243927 27366 solver.cpp:241]     Train net output #0: loss = 1.8691 (* 1 = 1.8691 loss)
I1223 13:24:34.243944 27366 sgd_solver.cpp:119] Iteration 200, lr = 0.000993022
I1223 13:24:50.235770 27366 solver.cpp:222] Iteration 225 (1.56336 iter/s, 15.9912s/25 iters), loss = 1.58644
I1223 13:24:50.235843 27366 solver.cpp:241]     Train net output #0: loss = 1.58644 (* 1 = 1.58644 loss)
I1223 13:24:50.235862 27366 sgd_solver.cpp:119] Iteration 225, lr = 0.00111499
I1223 13:25:10.725697 27366 solver.cpp:222] Iteration 250 (1.22016 iter/s, 20.4891s/25 iters), loss = 1.63825
I1223 13:25:10.725915 27366 solver.cpp:241]     Train net output #0: loss = 1.63825 (* 1 = 1.63825 loss)
I1223 13:25:10.725932 27366 sgd_solver.cpp:119] Iteration 250, lr = 0.00123662
I1223 13:25:26.800510 27366 solver.cpp:222] Iteration 275 (1.55531 iter/s, 16.074s/25 iters), loss = 1.67721
I1223 13:25:26.800573 27366 solver.cpp:241]     Train net output #0: loss = 1.67721 (* 1 = 1.67721 loss)
I1223 13:25:26.800599 27366 sgd_solver.cpp:119] Iteration 275, lr = 0.00135791
I1223 13:25:46.090016 27366 solver.cpp:222] Iteration 300 (1.2961 iter/s, 19.2887s/25 iters), loss = 1.85079
I1223 13:25:46.090245 27366 solver.cpp:241]     Train net output #0: loss = 1.85079 (* 1 = 1.85079 loss)
I1223 13:25:46.090265 27366 sgd_solver.cpp:119] Iteration 300, lr = 0.00147887
I1223 13:26:06.994981 27366 solver.cpp:222] Iteration 325 (1.19595 iter/s, 20.9039s/25 iters), loss = 1.85818
I1223 13:26:06.995045 27366 solver.cpp:241]     Train net output #0: loss = 1.85818 (* 1 = 1.85818 loss)
I1223 13:26:06.995064 27366 sgd_solver.cpp:119] Iteration 325, lr = 0.00159948
I1223 13:26:17.607686 27366 solver.cpp:451] Snapshotting to binary proto file models_compression/alexnet/inq90_2_iter_338.caffemodel
I1224 00:40:32.832237 27366 sgd_solver.cpp:343] Snapshotting solver state to binary proto file models_compression/alexnet/inq90_2_iter_338.solverstate
I1224 00:41:37.992854 27366 solver.cpp:298] Optimization stopped early.
*** Aborted at 1514143747 (unix time) try "date -d @1514143747" if you are using GNU date ***
PC: @     0x7f5f983d8705 __pthread_cond_wait
*** SIGTERM (@0x3ed0000abc3) received by PID 27366 (TID 0x7f5fab32c740) from PID 43971; stack trace: ***
    @     0x7f5f983dc130 (unknown)
    @     0x7f5f983d8705 __pthread_cond_wait
    @     0x7f5faa7493d4 boost::condition_variable::wait()
    @     0x7f5fa0ca28d4 boost::thread::join_noexcept()
    @     0x7f5faa73475a caffe::InternalThread::StopInternalThread()
    @     0x7f5faa74ced2 caffe::NCCL<>::Run()
    @           0x40adef train()
    @           0x4082ec main
    @     0x7f5f9802daf5 __libc_start_main
    @           0x408bf5 (unknown)
